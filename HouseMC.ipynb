{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House, M.C.\n",
    "### Generation of random text that sounds like Dr. House"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Packages\n",
    "import urllib.request  # for grabbing the text off the internet\n",
    "import os     # to help save the files of text\n",
    "import re     # for replacement via regex (which I've never been good with TBH)\n",
    "import nltk   # for splitting text into words\n",
    "import random # for picking random words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a directory to store episode transcripts (if it doesn't already exist)\n",
    "try:\n",
    "    os.mkdir('texts')\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "# Get a list of transcript filenames\n",
    "with open(\"filenames.txt\", \"r\") as namesfile:\n",
    "    filenames = namesfile.readlines()\n",
    "filenames = [name.strip('\\n') for name in filenames]\n",
    "\n",
    "# Store the episode transcripts locally (if they don't already exist)\n",
    "# WARNING -- There are 177 files, totaling 36.2 MB.\n",
    "for name in filenames:\n",
    "    if not (os.path.isfile('texts/' + name)):\n",
    "        urllib.request.urlretrieve(\n",
    "            'http://community.livejournal.com/clinic_duty/' + name, \n",
    "            'texts/' + name\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Characters that will need to be removed or altered\n",
    "replacements = {\n",
    "    \"&rsquo;\" : \"'\",\n",
    "    \"&#39;\" : \"\",\n",
    "    \"&hellip;\" : \"...\",\n",
    "    \"\\\\\" : \"\",\n",
    "    '“' : '',\n",
    "    '”' : '',\n",
    "    \"’\" : \"'\",\n",
    "    '\"' : '',\n",
    "    '&lt;' : '',\n",
    "    \" -- \" : \" — \",\n",
    "    \" - \" : \" — \"\n",
    "}\n",
    "\n",
    "# Extract a list of the complete text spoken by Dr. House in every episode\n",
    "episodes = []\n",
    "for name in filenames:\n",
    "    with open('texts/' + name, 'r', encoding=\"utf8\") as myfile:\n",
    "        data = myfile.read().replace('\\n', '').split('<br />')\n",
    "        data = ' '.join([\n",
    "            re.sub( # eliminate any [text in brackets]\n",
    "                    r'\\[(.*?)\\]|\\<(.*?)\\>|\\((.*?)\\)', '', line\n",
    "                ).replace(  # The transcripts are very nonstandardized\n",
    "                    'GREG HOUSE: ', ''\n",
    "                ).replace(  # in the sense that there are four different\n",
    "                    'House: ', ''\n",
    "                ).replace(  # ways that House's lines may be marked\n",
    "                    'HOUSE: ', ''\n",
    "                ).replace(  # so we have to deal with each one :(\n",
    "                    'House : ', ''\n",
    "                )  \n",
    "            for line in data \n",
    "            if line.lower().startswith('house:') \n",
    "                or line.lower().startswith('greg house:')\n",
    "                or line.lower().startswith('house :')\n",
    "        ])\n",
    "        for key in replacements:\n",
    "            data = data.replace(key, replacements[key])\n",
    "        episodes.append(data)\n",
    "\n",
    "# Join all episodes into one large text string\n",
    "alltext = ' '.join(ep for ep in episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build the list of words used\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r\"\\w+|[^\\w\\s]+\")\n",
    "stream = tokenizer.tokenize(alltext)\n",
    "\n",
    "# Build a list of word pairs used (AKA 2-grams)\n",
    "stream2 = [(stream[i], stream[i+1]) for i in range(len(stream)-1)]\n",
    "\n",
    "# A function that takes a word and randomly chooses a word to follow it\n",
    "def nextword(word):\n",
    "    return stream[\n",
    "        random.choice([\n",
    "                i for i, j in enumerate(stream) \n",
    "                if (j == word) & (i < len(stream)-1)\n",
    "            ]) + 1\n",
    "    ]\n",
    "\n",
    "# A function that takes a pair of words and \n",
    "# randomly chooses a word to follow them\n",
    "def nextpair(pair):\n",
    "    return stream2[\n",
    "        random.choice([\n",
    "                i for i, j in enumerate(stream2) \n",
    "                if (j == pair) & (i < len(stream2)-2)\n",
    "            ]) + 1\n",
    "    ]\n",
    "\n",
    "# To clean up the generated text\n",
    "replacements = {\n",
    "    ' .' : '.',\n",
    "    ' ,' : ',',\n",
    "    \" ' \" : \"'\",\n",
    "    \" ?\" : \"?\",\n",
    "    \" !\" : \"!\",\n",
    "    \" ’ \" : \"’\",\n",
    "    \" ; \" : \"; \",\n",
    "    \" ‘ \" : \" ‘\",\n",
    "    \"$ \" : \"$\",\n",
    "    \" …\" : \"...\",\n",
    "    \" - \" : \"-\"\n",
    "}\n",
    "def postprocess(text):\n",
    "    for key in replacements:\n",
    "        text = text.replace(key, replacements[key])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I've got about her in the other can get it could use toe, I... I thought your blood.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate text using single words (not very good)\n",
    "words = [random.choice([word for word in stream if word[0].isupper()])]\n",
    "while (\n",
    "    (words[-1] != '.') & (words[-1] != '!') & (words[-1] != '?')\n",
    ") | (len(words) < 10):\n",
    "    words.append(nextword(words[-1]))\n",
    "postprocess(' '.join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Leukoencephalopathy? What does it matter what it looks like.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate text using 2-grams:\n",
    "pair = random.choice([pair for pair in stream2 if pair[0][0].isupper()])\n",
    "words = [pair[0], pair[1]]\n",
    "while (\n",
    "    (words[-1] != '.') & (words[-1] != '!') & (words[-1] != '?')\n",
    ") | (len(words) < 10):\n",
    "    pair = nextpair(pair)\n",
    "    words.append(pair[-1])\n",
    "postprocess(' '.join(words))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
